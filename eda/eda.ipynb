{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import contractions\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "stop_words_list = [\n",
    "    \"a\",\n",
    "    \"an\",\n",
    "    \"a\",\n",
    "    \"the\",\n",
    "    \"and\",\n",
    "    \"at\",\n",
    "    \"by\",\n",
    "    \"to\",\n",
    "    \"in\",\n",
    "    \"out\",\n",
    "    \"y\",\n",
    "    \"are\",\n",
    "    \"is\",\n",
    "    \"as\",\n",
    "    \"s\",\n",
    "    \"t\",\n",
    "    \"for\",\n",
    "]\n",
    "\n",
    "\n",
    "def remove_http_links(text):\n",
    "    return re.sub(r\"http\\S+|www.\\S+\", \" \", text)\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\", \" \", text)\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def lemmatize_word(word, pos_tag):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word, pos=pos_tag)\n",
    "\n",
    "\n",
    "def replace_digits(match):\n",
    "    char_map = {\"1\": \"i\", \"0\": \"o\", \"3\": \"e\"}\n",
    "    return char_map[match.group(0)]\n",
    "\n",
    "\n",
    "def replace_digit_with_letter(text):\n",
    "    \"\"\"\n",
    "    Transforms specified digits within a given string 'text' into their respective letter counterparts\n",
    "    commonly used in leetspeak or similar stylizations.\n",
    "\n",
    "    This function targets the digits '1', '0', and '3' when they are sandwiched between alphabetic characters\n",
    "    and replaces them with 'i', 'o', and 'e', respectively.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"(?<=[a-zA-Z])[103](?=[a-zA-Z])\", replace_digits, text)\n",
    "\n",
    "\n",
    "def reduce_repeated_letters(text):\n",
    "    \"\"\"\n",
    "    Compresses sequences of identical letters occurring more than twice in a row within a given string 'text'\n",
    "    to a single instance of that letter.\n",
    "\n",
    "    This function is particularly useful in normalizing text with exaggerated letter repetitions, commonly found in\n",
    "    informal communication like social media posts or text messages.\n",
    "\n",
    "    Example:\n",
    "        heeeello -> hello\n",
    "        worlllld -> world\n",
    "    \"\"\"\n",
    "    return re.sub(r\"(.)\\1{2,}\", r\"\\1\", text)\n",
    "\n",
    "\n",
    "def reduce_haha(text):\n",
    "    return re.sub(r\"\\bhaha\\w*\\b\", \"haha\", text)\n",
    "\n",
    "\n",
    "def replace_hyphens_with_spaces(text):\n",
    "    \"\"\"\n",
    "    Replaces all hyphens '-' in the string with spaces ' '.\n",
    "    \"\"\"\n",
    "    return text.replace(\"-\", \" \")\n",
    "\n",
    "\n",
    "def remove_non_word_characters(text):\n",
    "    \"\"\"\n",
    "    Removes all characters from the string that are not alphanumeric (letters and numbers) or whitespace.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "\n",
    "def replace_specific_word(word_to_replace, new_word, text):\n",
    "    \"\"\"\n",
    "    Replaces occurrences of a specified word with a new word in the given text.\n",
    "    This function targets only standalone instances of the specified word,\n",
    "    not parts of other words.\n",
    "    \"\"\"\n",
    "    pattern = r\"\\b{}\\b\".format(re.escape(word_to_replace))\n",
    "    return re.sub(pattern, new_word, text)\n",
    "\n",
    "\n",
    "def remove_spaces_from_spaced_words(text):\n",
    "    pattern = r\"(?:\\b\\w\\s){2,}\\w\\b\"\n",
    "\n",
    "    def replace_spaces(match):\n",
    "        return match.group().replace(\" \", \"\")\n",
    "\n",
    "    return re.sub(pattern, replace_spaces, text)\n",
    "\n",
    "\n",
    "def remove_digits(text):\n",
    "    \"\"\"\n",
    "    Removes all digits from the string.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    \"\"\"\n",
    "    Normalizes whitespace in the string, replacing multiple consecutive whitespace characters with a single space,\n",
    "    and trims leading and trailing whitespace.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "\n",
    "def filter_text(text):\n",
    "    text = text.lower()\n",
    "    text = remove_http_links(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = replace_digit_with_letter(text)\n",
    "    text = reduce_repeated_letters(text)\n",
    "    text = reduce_haha(text)\n",
    "    text = replace_hyphens_with_spaces(text)\n",
    "    text = contractions.fix(text)\n",
    "    text = remove_non_word_characters(text)\n",
    "    text = replace_specific_word(\"fck\", \"fuck\", text)\n",
    "    text = remove_digits(text)\n",
    "    text = remove_spaces_from_spaced_words(text)\n",
    "    tokens = normalize_whitespace(text).strip()\n",
    "\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words_list]\n",
    "\n",
    "    pos_tags = nltk.pos_tag(filtered_tokens)\n",
    "    lemmatized_tokens = [\n",
    "        lemmatize_word(word, get_wordnet_pos(pos)) for word, pos in pos_tags\n",
    "    ]\n",
    "\n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = \"             hello           world                   aaaaaaaaaaaaa           dddddddddddddddd             rrrrrrrrrrr  rr  \"\n",
    "print(normalize_whitespace(text))\n",
    "print(normalize_whitespace(text).strip().split())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e174eb846b8daef"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"I'm\", 'NNP'), ('tired', 'VBD')]\n",
      "[\"I'm\", 'tire']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = \"I'm tired\".strip().split()\n",
    "pos_tags = nltk.pos_tag(filtered_tokens)\n",
    "print(pos_tags)\n",
    "lemmatized_tokens = [\n",
    "    lemmatize_word(word, get_wordnet_pos(pos)) for word, pos in pos_tags\n",
    "]\n",
    "\n",
    "print(lemmatized_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T14:31:59.492391545Z",
     "start_time": "2023-12-07T14:31:59.329705836Z"
    }
   },
   "id": "2b738a897efc68f8"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'She is running every day.'\n",
      "Tags: [('She', 'PRP'), ('is', 'VBZ'), ('running', 'VBG'), ('every', 'DT'), ('day', 'NN'), ('.', '.')]\n",
      "['She', 'be', 'run', 'every', 'day', '.']\n",
      "Sentence: 'He bought running shoes.'\n",
      "Tags: [('He', 'PRP'), ('bought', 'VBD'), ('running', 'VBG'), ('shoes', 'NNS'), ('.', '.')]\n",
      "['He', 'buy', 'run', 'shoe', '.']\n",
      "Sentence: 'The cat is sitting on the mat.'\n",
      "Tags: [('The', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN'), ('.', '.')]\n",
      "['The', 'cat', 'be', 'sit', 'on', 'the', 'mat', '.']\n",
      "Sentence: 'It was a bright, sunny day.'\n",
      "Tags: [('It', 'PRP'), ('was', 'VBD'), ('a', 'DT'), ('bright', 'JJ'), (',', ','), ('sunny', 'JJ'), ('day', 'NN'), ('.', '.')]\n",
      "['It', 'be', 'a', 'bright', ',', 'sunny', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"She is running every day.\",\n",
    "    \"He bought running shoes.\",\n",
    "    \"The cat is sitting on the mat.\",\n",
    "    \"It was a bright, sunny day.\",\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    print(f\"Sentence: '{sentence}'\")\n",
    "    print(\"Tags:\", tagged)\n",
    "    lemmatized_tokens = [\n",
    "        lemmatize_word(word, get_wordnet_pos(pos)) for word, pos in tagged\n",
    "    ]\n",
    "\n",
    "    print(lemmatized_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T14:33:36.266683603Z",
     "start_time": "2023-12-07T14:33:36.223716055Z"
    }
   },
   "id": "4aaa53ebbb6047ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "52d81b6508e1b463"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
